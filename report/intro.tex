\section{Introduction}
Recognition of named entities in biomedical text is a critical step for information extraction tasks such as drug–drug interaction detection~\cite{ddi-corpus}. In particular, identifying pharmacological substances (drug names, brand names, drug classes, etc.) in literature enables downstream systems to analyze drug safety and pharmacovigilance. This work addresses the problem of Named Entity Recognition and Classification (NERC) for drug-related entities, using the DDI corpus~\cite{ddi-corpus} as a benchmark dataset. The DDI corpus is a collection of 1,025 documents (792 DrugBank drug description texts and 233 MedLine abstracts) manually annotated with drug mentions and their types~\cite{ddi-corpus}. Each drug mention is categorized into one of four entity types: \textbf{drug} (generic drug names), \textbf{brand} (brand or trade names), \textbf{group} (pharmacological classes or categories of drugs), and \textbf{drug\_n} (active substances not approved for human use, e.g. toxins or experimental compounds)~\cite{ddi-lessons}. The goal of this task is to locate all such entities in text and correctly label them with their type.

Traditional named entity recognition in biomedical domains has employed a range of methods from rule-based approaches to statistical sequence models. Given the complexity of language and variability of drug nomenclature, machine learning approaches with robust feature engineering have become the standard. In this work, we adopt a sequence tagging approach with the popular BIO encoding scheme (Begin, Inside, Outside) to mark entity boundaries. Each token in a sentence is assigned a label B-type if it begins an entity of a given type, I-type if it continues an entity, or O if it is not part of any entity. For example, in the sentence \textit{"The fluoroquinolones for urinary tract infections: a review."}, the token "fluoroquinolones" would be labeled \textbf{B-group} (beginning a drug group entity), while subsequent tokens like "urinary" are labeled \textbf{O} (outside any entity).

We focus on two probabilistic sequence modeling algorithms for this task: a Conditional Random Field (CRF) and a simpler Naïve Bayes (NB) classifier. CRFs are undirected graphical models widely used for NER because they can capture arbitrary overlapping features of the input and model dependencies between output labels in a sequence~\cite{crf-tutorial}. In essence, a CRF computes the conditional probability of a label sequence given an input sequence and includes features that consider neighboring labels, thereby enforcing consistency (e.g. preventing an illegal sequence like I-drug appearing without a preceding B-drug). In contrast, a Naïve Bayes model treats each token independently (with conditional independence assumptions among features) and assigns labels based only on the token's own features (context is included as features but the model does not enforce sequence consistency in predictions). While simpler, NB can serve as a baseline to evaluate the benefit of modeling label transitions.

This report details our implemented solution. In Section~\ref{sec:methodology}, we describe the features engineered for representing each token to the learning algorithms, and outline the learning algorithms (CRF and NB) and their configurations. In Section~\ref{sec:implementation}, we explain how the system was realized, including parsing of the XML corpus, the integration of external knowledge sources, and training procedures. Results and evaluation are then presented in Section~\ref{sec:experiments}, with performance metrics on the development and test sets. We include both quantitative results – with precision, recall, and F1-score for each entity type – and qualitative analysis of common errors. Finally, the conclusion summarizes our findings and discusses future improvements, such as employing neural network models or more comprehensive lexicons, which are suggested by our results but were beyond the scope of this assignment.
